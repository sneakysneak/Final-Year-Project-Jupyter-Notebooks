{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "In this comparison a Simple Recurrent Neural Network, an LSTM, a GRU and lastly a BiDirectional LSTM model being evaluated, all of them on Glove Embeddings.\n",
    "\n",
    "All read file variables (like train.csv, embeddings files etc) are set to relative path, which means just drag the competition files in to the program's folder and it reads in them automatically. \n",
    "Furthermore, I turned off the support for AMD Radeon GPUs, turn it on if necessary (for quicker computing on Radeon machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sneakysneak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/sneakysneak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/sneakysneak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/sneakysneak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/sneakysneak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/sneakysneak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# support for AMD Radeon GPU - if you run this on AMD Radeon GPU computer, then use it\n",
    "\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "# keras layers, tokenizer, model, sequential etc.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, SimpleRNN, RNN, LSTM, GRU, Embedding, Dropout, Activation, Flatten, Conv1D, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import tensorflow as tf\n",
    "# time for idle the system after deleting models and embedding to test in one notebook\n",
    "import time\n",
    "# linear algebra\n",
    "import numpy as np \n",
    "# data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd \n",
    "# monitor loading time where it is supported\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import gc\n",
    "\n",
    "random_seed = 63445\n",
    "lsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"/Users/sneakysneak/Downloads/quora_dataset/train.csv\")\n",
    "# test_df = pd.read_csv(\"/Users/sneakysneak/Downloads/quora_dataset/test.csv\")]\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the data frame to train and val\n",
    "train_data_frame, value_data_frame = train_test_split(train_data_frame, test_size=0.1, random_state=2018)\n",
    "\n",
    "## configuration values \n",
    "# 300 \n",
    "embedding_size = 300 # the size of each word vector\n",
    "# 50000\n",
    "max_features = 50000 # the size of unique words in use - the number of rows in the embedding vector\n",
    "# 100 \n",
    "max_length_question = 100 # the size of the number of words in each question\n",
    "\n",
    "## first fill all missing values up\n",
    "train_X = train_data_frame[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = value_data_frame[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_data_frame[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## tokenize with Keras\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## sentence padding\n",
    "train_X = pad_sequences(train_X, maxlen=max_length_question)\n",
    "val_X = pad_sequences(val_X, maxlen=max_length_question)\n",
    "test_X = pad_sequences(test_X, maxlen=max_length_question)\n",
    "\n",
    "## save the target values as train_y and val_y\n",
    "train_y = train_data_frame['target'].values\n",
    "val_y = value_data_frame['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  In the for loop with the  help of the  tqdm library (a command line “display interface” which displays time and bar when something is loading). Values splits by space. Word set to 0 values, counting from 0. Coefs is a numpy array conversion of values variable with the float32 data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [05:08, 7123.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings glove text file\n",
    "embeddings_index = {}\n",
    "# glove_emb = open('/Users/sneakysneak/Downloads/quora_dataset/glove.840B.300d/glove.840B.300d.txt')\n",
    "glove_emb = open('glove.840B.300d.txt')\n",
    "\n",
    "for line in tqdm(glove_emb):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "glove_emb.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this function creating a numpy array filled with 300 zeros. Text variable’s length split to 30. Embeds variable iterates through on the empty_emb 300 zeros and adds them in 30 minus the number of embeds. So, it’s “filling” them up and returns the numpy array with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to embeddings\n",
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = text[:-1].split()[:30]\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
    "    return np.array(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this new variable val_vects populates this new array with the np.array about text_to_array method from above, with the train_df dummy variable which is the train.csv. Applying tqdm loading screen and the “qhestom text” tab ln the csv file and iterates through on 3000 at once. Val_y creates an array from the “target” tab of the csv, 3000 at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 5564.62it/s]\n"
     ]
    }
   ],
   "source": [
    "val_vects = np.array([text_to_array(train_data_frame) for train_data_frame in tqdm(value_data_frame[\"question_text\"][:3000])])\n",
    "val_y = np.array(value_data_frame[\"target\"][:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define batch_size variable. Define batch_gen fucntion with the train_df variable, which is the train.csv file. This function creates batches from the train.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def batch_gen(train_data_frame):\n",
    "    n_batches = math.ceil(len(train_data_frame) / batch_size)\n",
    "    while True: \n",
    "        train_data_frame = train_data_frame.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_data_frame.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_data_frame[\"target\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define an LSTM model in keras. Firstly, define a Sequential model which is a linear stack of layers. In this example, the .add() method will be used. Added to LSTM layers, the first layer (only the first, the rest can do automatic shape inference) must have an input shape argument. In the first layer the input shape is (*,30, 300) sort of (batch_size, 30,300). The output of thhe lstm_1 layer is  (None, 30, 64). So, the first dimension’s output is None because the batch size is unknown in this case.\n",
    "##### Dense layer, via the argument they support their input shape. The activation is the activation function which is passed to this argument, in this case “sigmoid”.\n",
    "##### As it was  mentioned above, the input_shape is automatic on the rest of the layers, their output is similar, only the first argument (64) will be the last dimension in their output. In the dense layer it is (batch_size, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_560.0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 64)            93440     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 126,529\n",
      "Trainable params: 126,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input shape (*,30, 300) sort of (batch_size, 30,300) \n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(30, 300)))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              object\n",
       "question_text    object\n",
       "target            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_frame.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The mg is the variable instantiated from the batch_gen function above, the dummy variable in it is the train_df, which is the train.csv dataset. \n",
    "##### The .fit_generator inbuilt keras function, which trains the model on the dataset batch-by-batch. The mg comes first, after the number of epochs, and the step_per_epoch, which is the total number of steps in batches of samples before declaring one epoch is finished and between the next epoch starting.\n",
    "##### The validation_data is an immutable sequence , a tuple, val_vects, val_y, so the vector texts and the target values, 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2593 of 5988 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 5582 of 5988 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 1875 of 2569 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1000/1000 [==============================] - 124s 124ms/step - loss: 0.1456 - acc: 0.9472 - val_loss: 0.1207 - val_acc: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab1d2d518>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_data_frame)\n",
    "model.fit_generator(mg, epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a dictionary variable as thresholds. Using the inbuilt keras .predict function with val_vects, defined batch size and verbose is 1. The for loop iterates through a numpy array trying to determine the best f1_score and its threshold value. In the last 3 lines, prints out the best threshold value, which is 0.33 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048/3000 [===================>..........] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 1730 of 2535 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 5s 2ms/step\n",
      "F1 score at threshold 0.1 is 0.5185185185185185\n",
      "F1 score at threshold 0.11 is 0.5280728376327769\n",
      "F1 score at threshold 0.12 is 0.5318818040435459\n",
      "F1 score at threshold 0.13 is 0.5352564102564102\n",
      "F1 score at threshold 0.14 is 0.5392156862745099\n",
      "F1 score at threshold 0.15 is 0.5509181969949917\n",
      "F1 score at threshold 0.16 is 0.559726962457338\n",
      "F1 score at threshold 0.17 is 0.5674255691768826\n",
      "F1 score at threshold 0.18 is 0.5796064400715563\n",
      "F1 score at threshold 0.19 is 0.5843920145190562\n",
      "F1 score at threshold 0.2 is 0.5897435897435898\n",
      "F1 score at threshold 0.21 is 0.5921787709497207\n",
      "F1 score at threshold 0.22 is 0.6011342155009453\n",
      "F1 score at threshold 0.23 is 0.6080305927342257\n",
      "F1 score at threshold 0.24 is 0.6061776061776062\n",
      "F1 score at threshold 0.25 is 0.6093749999999999\n",
      "F1 score at threshold 0.26 is 0.6086956521739131\n",
      "F1 score at threshold 0.27 is 0.6067864271457085\n",
      "F1 score at threshold 0.28 is 0.6116700201207245\n",
      "F1 score at threshold 0.29 is 0.6141414141414141\n",
      "F1 score at threshold 0.3 is 0.6138211382113821\n",
      "F1 score at threshold 0.31 is 0.6090534979423868\n",
      "F1 score at threshold 0.32 is 0.606694560669456\n",
      "F1 score at threshold 0.33 is 0.606315789473684\n",
      "F1 score at threshold 0.34 is 0.6101694915254237\n",
      "F1 score at threshold 0.35 is 0.6167023554603854\n",
      "F1 score at threshold 0.36 is 0.6090712742980562\n",
      "F1 score at threshold 0.37 is 0.6130434782608696\n",
      "F1 score at threshold 0.38 is 0.6157205240174672\n",
      "F1 score at threshold 0.39 is 0.6194690265486725\n",
      "F1 score at threshold 0.4 is 0.6219239373601789\n",
      "F1 score at threshold 0.41 is 0.618510158013544\n",
      "F1 score at threshold 0.42 is 0.6195899772209568\n",
      "F1 score at threshold 0.43 is 0.6203703703703703\n",
      "F1 score at threshold 0.44 is 0.6232558139534883\n",
      "F1 score at threshold 0.45 is 0.6261682242990654\n",
      "F1 score at threshold 0.46 is 0.6276346604215456\n",
      "F1 score at threshold 0.47 is 0.624113475177305\n",
      "F1 score at threshold 0.48 is 0.6298076923076922\n",
      "F1 score at threshold 0.49 is 0.6280193236714977\n",
      "F1 score at threshold 0.5 is 0.6308068459657702\n",
      "Best threshold:  0.5\n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "\n",
    "pred_lstm_val_y = model.predict([val_vects], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_lstm_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In here, the model is deleted and gc is the garbage collector collects the leftover of the remaining model, and the program idle for 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This model is a simple recurrent nn. It has only 1 layer, the input shape is the same. However, return_sequences is set to False, otherwise cannot use the fit_generator batch generating method. SimpleRNN is a simpler model, it is kind of a base model of recurrent neural networks. The LSTM’s 130-140 seconds average training time per epoch here reduces to 22-26 seconds, but it’s reflecting on its f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                23360     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 23,425\n",
      "Trainable params: 23,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, return_sequences=False, input_shape=(30, 300)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 30, 300), (3000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vects.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.1730 - acc: 0.9387 - val_loss: 0.1385 - val_acc: 0.9370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab2515a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_data_frame)\n",
    "model.fit_generator(mg, epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 1s 348us/step\n",
      "F1 score at threshold 0.1 is 0.5565529622980251\n",
      "F1 score at threshold 0.11 is 0.5595667870036101\n",
      "F1 score at threshold 0.12 is 0.5636363636363637\n",
      "F1 score at threshold 0.13 is 0.562043795620438\n",
      "F1 score at threshold 0.14 is 0.5567765567765568\n",
      "F1 score at threshold 0.15 is 0.5588235294117647\n",
      "F1 score at threshold 0.16 is 0.5598526703499078\n",
      "F1 score at threshold 0.17 is 0.5598526703499078\n",
      "F1 score at threshold 0.18 is 0.5619223659889095\n",
      "F1 score at threshold 0.19 is 0.5629629629629629\n",
      "F1 score at threshold 0.2 is 0.5602968460111317\n",
      "F1 score at threshold 0.21 is 0.5613382899628253\n",
      "F1 score at threshold 0.22 is 0.5634328358208955\n",
      "F1 score at threshold 0.23 is 0.5666041275797373\n",
      "F1 score at threshold 0.24 is 0.569811320754717\n",
      "F1 score at threshold 0.25 is 0.5719696969696969\n",
      "F1 score at threshold 0.26 is 0.5725190839694657\n",
      "F1 score at threshold 0.27 is 0.5758157389635316\n",
      "F1 score at threshold 0.28 is 0.5780346820809249\n",
      "F1 score at threshold 0.29 is 0.5802707930367504\n",
      "F1 score at threshold 0.3 is 0.5775193798449613\n",
      "F1 score at threshold 0.31 is 0.58203125\n",
      "F1 score at threshold 0.32 is 0.5838264299802761\n",
      "F1 score at threshold 0.33 is 0.5849802371541502\n",
      "F1 score at threshold 0.34 is 0.5861386138613861\n",
      "F1 score at threshold 0.35 is 0.5851703406813626\n",
      "F1 score at threshold 0.36 is 0.591093117408907\n",
      "F1 score at threshold 0.37 is 0.5950413223140496\n",
      "F1 score at threshold 0.38 is 0.5961945031712474\n",
      "F1 score at threshold 0.39 is 0.5783664459161147\n",
      "F1 score at threshold 0.4 is 0.5727699530516432\n",
      "F1 score at threshold 0.41 is 0.5522788203753352\n",
      "F1 score at threshold 0.42 is 0.47500000000000003\n",
      "F1 score at threshold 0.43 is 0.35842293906810035\n",
      "F1 score at threshold 0.44 is 0.28793774319066145\n",
      "F1 score at threshold 0.45 is 0.2540983606557377\n",
      "F1 score at threshold 0.46 is 0.22784810126582278\n",
      "F1 score at threshold 0.47 is 0.18340611353711792\n",
      "F1 score at threshold 0.48 is 0.15454545454545454\n",
      "F1 score at threshold 0.49 is 0.12093023255813952\n",
      "F1 score at threshold 0.5 is 0.12093023255813952\n",
      "Best threshold:  0.38\n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "\n",
    "pred_simple_rnn_val_y = model.predict([val_vects], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_simple_rnn_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The last model is the gated recurrent unit GRU.  Two layers were defined, the parameters are different from LSTM or SimplRNN. The difference in a nutshell between LSTM and GRU is GRU has two gates (update and reset) while LSTM has three (input, output, forget). Furthermore, GRU slightly faster 113-133 seconds average/epoch training time, whereas LSTM’s 130-140 seconds. The best f1 score of GRU is 0.63 while LSTM 0.70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 30, 32)            31968     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 38,241\n",
      "Trainable params: 38,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True,  input_shape=(30, 300)))\n",
    "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.build()\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2716 of 5320 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9403"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2114 of 2116 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1000/1000 [==============================] - 92s 92ms/step - loss: 0.1730 - acc: 0.9403 - val_loss: 0.1264 - val_acc: 0.9443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab2500da0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_data_frame)\n",
    "model.fit_generator(mg, epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 4s 1ms/step\n",
      "F1 score at threshold 0.1 is 0.5385878489326764\n",
      "F1 score at threshold 0.11 is 0.5469798657718121\n",
      "F1 score at threshold 0.12 is 0.5506756756756758\n",
      "F1 score at threshold 0.13 is 0.552901023890785\n",
      "F1 score at threshold 0.14 is 0.5542168674698795\n",
      "F1 score at threshold 0.15 is 0.562390158172232\n",
      "F1 score at threshold 0.16 is 0.5668449197860963\n",
      "F1 score at threshold 0.17 is 0.5771324863883848\n",
      "F1 score at threshold 0.18 is 0.5824175824175823\n",
      "F1 score at threshold 0.19 is 0.5878003696857671\n",
      "F1 score at threshold 0.2 is 0.5880149812734082\n",
      "F1 score at threshold 0.21 is 0.5913370998116761\n",
      "F1 score at threshold 0.22 is 0.5900383141762452\n",
      "F1 score at threshold 0.23 is 0.5941747572815533\n",
      "F1 score at threshold 0.24 is 0.5984251968503937\n",
      "F1 score at threshold 0.25 is 0.5976095617529881\n",
      "F1 score at threshold 0.26 is 0.6008064516129031\n",
      "F1 score at threshold 0.27 is 0.595482546201232\n",
      "F1 score at threshold 0.28 is 0.5933609958506224\n",
      "F1 score at threshold 0.29 is 0.5953878406708596\n",
      "F1 score at threshold 0.3 is 0.5919661733615221\n",
      "F1 score at threshold 0.31 is 0.5935483870967742\n",
      "F1 score at threshold 0.32 is 0.5982532751091704\n",
      "F1 score at threshold 0.33 is 0.5916114790286976\n",
      "F1 score at threshold 0.34 is 0.5906040268456376\n",
      "F1 score at threshold 0.35 is 0.5909090909090909\n",
      "F1 score at threshold 0.36 is 0.5839080459770114\n",
      "F1 score at threshold 0.37 is 0.5906976744186047\n",
      "F1 score at threshold 0.38 is 0.5971563981042655\n",
      "F1 score at threshold 0.39 is 0.5845410628019324\n",
      "F1 score at threshold 0.4 is 0.5770171149144254\n",
      "F1 score at threshold 0.41 is 0.5635910224438903\n",
      "F1 score at threshold 0.42 is 0.5555555555555555\n",
      "F1 score at threshold 0.43 is 0.5558441558441557\n",
      "F1 score at threshold 0.44 is 0.5531914893617021\n",
      "F1 score at threshold 0.45 is 0.5537634408602151\n",
      "F1 score at threshold 0.46 is 0.5369863013698629\n",
      "F1 score at threshold 0.47 is 0.5168539325842697\n",
      "F1 score at threshold 0.48 is 0.5227272727272727\n",
      "F1 score at threshold 0.49 is 0.5242165242165242\n",
      "F1 score at threshold 0.5 is 0.5159420289855073\n",
      "Best threshold:  0.26\n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "\n",
    "pred_gru_val_y = model.predict([val_vects], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_gru_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM\n",
    "- Sequential means 1 in 1 out\n",
    "- LSTM(64 - spits out in 64 dimension\n",
    "- takes in the input shape's size\n",
    "- the second LSTM layer takes in 64 dimenson\n",
    "- spits out in 64\n",
    "- dense(8 takes in 64D\n",
    "- spits out 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 30, 128)           186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 1032      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 286,737\n",
      "Trainable params: 286,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5),\n",
    "                      input_shape=(30,300)))  # return_sequences=True , stateful=True\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(8))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2432 of 12343 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 6000 of 12343 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 9063 of 12343 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 11904 of 12343 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9491"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 3032 of 5120 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 3767 of 5120 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1000/1000 [==============================] - 236s 236ms/step - loss: 0.1349 - acc: 0.9491 - val_loss: 0.1149 - val_acc: 0.9513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa88d1390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_data_frame)\n",
    "model.fit_generator(mg, epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2182 of 5086 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048/3000 [===================>..........] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 3220 of 5086 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 9s 3ms/step\n",
      "F1 score at threshold 0.1 is 0.5537974683544303\n",
      "F1 score at threshold 0.11 is 0.5584415584415585\n",
      "F1 score at threshold 0.12 is 0.5681063122923589\n",
      "F1 score at threshold 0.13 is 0.5816326530612245\n",
      "F1 score at threshold 0.14 is 0.5808695652173913\n",
      "F1 score at threshold 0.15 is 0.5945945945945946\n",
      "F1 score at threshold 0.16 is 0.5947955390334573\n",
      "F1 score at threshold 0.17 is 0.5996204933586337\n",
      "F1 score at threshold 0.18 is 0.5996131528046422\n",
      "F1 score at threshold 0.19 is 0.6123260437375745\n",
      "F1 score at threshold 0.2 is 0.6178861788617886\n",
      "F1 score at threshold 0.21 is 0.6239669421487603\n",
      "F1 score at threshold 0.22 is 0.628691983122363\n",
      "F1 score at threshold 0.23 is 0.6322580645161291\n",
      "F1 score at threshold 0.24 is 0.6301969365426697\n",
      "F1 score at threshold 0.25 is 0.6283185840707964\n",
      "F1 score at threshold 0.26 is 0.6202247191011236\n",
      "F1 score at threshold 0.27 is 0.6129032258064516\n",
      "F1 score at threshold 0.28 is 0.6179245283018868\n",
      "F1 score at threshold 0.29 is 0.6161137440758294\n",
      "F1 score at threshold 0.3 is 0.6150121065375302\n",
      "F1 score at threshold 0.31 is 0.6210268948655258\n",
      "F1 score at threshold 0.32 is 0.624078624078624\n",
      "F1 score at threshold 0.33 is 0.6281407035175879\n",
      "F1 score at threshold 0.34 is 0.6361323155216285\n",
      "F1 score at threshold 0.35 is 0.6307692307692307\n",
      "F1 score at threshold 0.36 is 0.6335078534031413\n",
      "F1 score at threshold 0.37 is 0.6351706036745408\n",
      "F1 score at threshold 0.38 is 0.6329787234042553\n",
      "F1 score at threshold 0.39 is 0.6287262872628726\n",
      "F1 score at threshold 0.4 is 0.6077348066298343\n",
      "F1 score at threshold 0.41 is 0.6072423398328691\n",
      "F1 score at threshold 0.42 is 0.6067415730337078\n",
      "F1 score at threshold 0.43 is 0.5982905982905983\n",
      "F1 score at threshold 0.44 is 0.5942857142857143\n",
      "F1 score at threshold 0.45 is 0.5878962536023055\n",
      "F1 score at threshold 0.46 is 0.5797101449275363\n",
      "F1 score at threshold 0.47 is 0.5747800586510264\n",
      "F1 score at threshold 0.48 is 0.5739644970414203\n",
      "F1 score at threshold 0.49 is 0.5654761904761905\n",
      "F1 score at threshold 0.5 is 0.5575757575757576\n",
      "Best threshold:  0.34\n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "\n",
    "pred_simple_rnn_val_y = model.predict([val_vects], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_simple_rnn_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
